<!DOCTYPE html>
<html lang="en">
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Shiwei Gan</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="" />
	<meta name="keywords" content="" />
	<meta name="author" content="" />

  <!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">

	<link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700" rel="stylesheet">
	
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link rel="stylesheet" href="https://i.icomoon.io/public/temp/c3b0292278/UntitledProject/style.css">

	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<!-- Flexslider  -->
	<link rel="stylesheet" href="css/flexslider.css">
	<!-- Flaticons  -->
	<link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
	<!-- Owl Carousel -->
	<link rel="stylesheet" href="css/owl.carousel.min.css">
	<link rel="stylesheet" href="css/owl.theme.default.min.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body>
	<div id="colorlib-page">
		<div class="container-wrap">
		<a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><i></i></a>
		<aside id="colorlib-aside" role="complementary" class="border js-fullheight">
			<div class="text-center">
				<!-- <div class="author-img" style="background-image: url(images/profile_formal.png);"></div>-->
				<a href="images/profile_formal.png" target="_blank">
			      	  <div class="author-img" style="background-image: url(images/profile_formal.png);"></div>
			    	</a>
				<h1 id="colorlib-logo"><a href="index.html">Shiwei Gan</a></h1>
				<span class="position"><a href="#">Machine Learning & Computer Vision</a></span>
			</div>
			<nav id="colorlib-main-menu" role="navigation" class="navbar">
				<div id="navbar" class="collapse">
					<ul>
						<!--<li class="active"><a href="#" data-nav-section="home">Home</a></li>-->
						<li><a href="#" data-nav-section="about">About</a></li>
						<li><a href="#" data-nav-section="blog">Research</a></li>
						<li><a href="#" data-nav-section="education">Education</a></li>
						<li><a href="#" data-nav-section="experience">Experience</a></li>
						
					</ul>
				</div>
			</nav>
			<div class="text-center social-media">
				<ul>
					<li><a href="mailto:sw@smail.nju.edu.cn?cc=gswycf@qq.com"><i class="icon-mail5"></i></a></li>
					<li><a href="static/shiweiCV.pdf" target="_blank"><i class="ai ai-cv"></i></a></li>
					<li><a href="https://scholar.google.com/citations?user=qXmzwiQAAAAJ&hl=zh-CN" target="_blank"><i class="ai ai-google-scholar-square"></i></a></li>
					<li><a href="https://www.linkedin.com/in/shiwei-gan-34491a18b/" target="_blank"><i class="icon-linkedin2"></i></a></li>
					<li><a href="https://github.com/gswycf" target="_blank"><i class="icon-github"></i></a></li>
					<li><a href="https://twitter.com/gswycf" target="_blank"><i class="icon-twitter2"></i></a></li>
				</ul>
			</div>


			<div class="colorlib-footer" style="position: absolute; bottom: 0;">
				<p><small>&copy; <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
Copyright <script>document.write(new Date().getFullYear());</script> All rights reserved. Made by <a href="https://colorlib.com" target="_blank">Colorlib</a>
<!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. --> </span> <span>Distributed by <a href="https://themewagon.com" target="_blank">ThemeWagon</a></span>
				<!--<ul>
					<li><a href="#"><i class="icon-facebook2"></i></a></li>
					<li><a href="#"><i class="icon-twitter2"></i></a></li>
					<li><a href="#"><i class="icon-instagram"></i></a></li>
					<li><a href="#"><i class="icon-linkedin2"></i></a></li>
				</ul>-->
			</div>

		</aside>

		<div id="colorlib-main">

			<section class="colorlib-about" data-section="about">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm">
								<div class="col-md-12">
									<div class="about-desc">
										<!--<span class="heading-meta">About Me</span>-->
										<h2 class="colorlib-heading">About Me</h2>
										<!--<p style="font-size: 18px;"><strong>Hi! I'm Shiwei Gan</strong> -->
										<p style="font-size: 16px; text-align: justify;">I am a PhD candidate at the Nanjing University, specializing in Deep Learning and Computer Vision. 
											I work under the supervision of Prof.Sanglu LU and  assistant professor Yafeng Yin.
											My PhD research centers on Sign Language Recognition and Sign Language Translation, encompassing two primary domains: Computer Vision and Multimodal Learning. 
											Broadly speaking, my investigations aim to convert video sequences into text, and to learn human acition representations from videos with CV model and NLP model.
											I am also interest in LVMs.
									</div>
								</div>
							</div>

						</div>
					</div>
				<div class="row row-bottom-padded-sm">
					<div class="news">
						<div class="col-md-12">
							<h2 class="colorlib-heading">News</h2>
							<ul>
								<li><b>May 2024:</b> One paper is accepted at CVPR '24 <a href="https://openreview.net/group?id=thecvf.com/CVPR/2024/Conference#tab-recent-activity" target="_blank">pdf</a></li> </li>
								<li><b>January 2024:</b> One paper is accepted at ACM IMWUT/Ubicomp'24 <a href="https://dl.acm.org/doi/10.1145/3631445" target="_blank">pdf</a></li>
								<li><b>August 2023:</b> One paper is accepted at IJCAI '23 <a href="https://www.ijcai.org/proceedings/2023/0085.pdf" target="_blank">pdf</a></li>
								<li><b>September 2023:</b> One paper is accepted at ACM MM '23 <a href="https://dl.acm.org/doi/10.1145/3581783.3611820" target="_blank">pdf</a></li>
								<li><b>October 2021:</b> One paper is accepted at ACM MM '21 <a href="https://dl.acm.org/doi/10.1145/3474085.3475577" target="_blank">pdf</a></li>
							</ul>
						</div>
					</div>
				</div>

				</div>
			</section>


<section class="colorlib-blog" data-section="blog">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3">
							<h2 class="colorlib-heading">Research</h2>
						</div>
					</div>
					<div class="row" style="display: flex;align-items: center;">
						<div class="col-md-4 col-sm-6">
							<div class="blog-entry">
								<a href="https://openreview.net/group?id=thecvf.com/CVPR/2024/Conference#tab-recent-activity" target="_blank" class="blog-img"><img src="images/scrub.png" class="img-responsive" alt="HTML5 Bootstrap Template by colorlib.com"></a>
							</div>
						</div>
						<div class="col-md-8 col-sm-6">
							<div class="blog-entry">
								
								<div class="desc">
									<h3><a href="https://openreview.net/group?id=thecvf.com/CVPR/2024/Conference#tab-recent-activity" target="_blank">Despite the recent success of sign language research, the widely adopted CNN-based backbones are mainly migrated from other computer vision tasks, 
										in which the contours and texture of objects are crucial for {identifying} objects. 
										They usually treat sign frames as grids and may fail to capture effective cross-region features. In fact, sign language tasks need to focus on the correlation of different regions in one frame and the interaction of different regions among adjacent frames for {identifying} a sign sequence. In this paper, we propose to represent a sign sequence as graphs and introduce a simple yet effective graph-based sign language processing architecture named SignGraph, to extract cross-region features at the graph level. SignGraph consists of two basic modules: Local Sign Graph ($LSG$) module for learning the correlation of \textbf{intra-frame cross-region}  features in one frame and Temporal Sign Graph ($TSG$) module for tracking the interaction of \textbf{inter-frame cross-region} features among adjacent frames. With $LSG$ and $TSG$, we build our model in a multiscale manner to ensure that the representation of nodes can capture cross-region features at different granularities. Extensive experiments on current public sign language datasets demonstrate the superiority of our SignGraph model. Our model achieves very
										competitive performances with the SOTA model, while not using any extra cues. Code and models are available at: \href{https://github.com/gswycf/SignGraph}{\textcolor{blue}{https://github.com/gswycf/SignGraph}}.
									</p>
								</div>
							</div>
						</div>
					</div>

					<div class="row" style="display: flex;align-items: center;">
						<div class="col-md-4 col-sm-6">
							<div class="blog-entry">
								<a href="https://www.ijcai.org/proceedings/2023/0085.pdf" target="_blank" class="blog-img"><img src="images/ddup.png" class="img-responsive" alt="HTML5 Bootstrap Template by colorlib.com"></a>
							</div>
						</div>
						<div class="col-md-8 col-sm-6">
							<div class="blog-entry">
								
								<div class="desc">
									<h3><a href="https://www.ijcai.org/proceedings/2023/0085.pdf" target="_blank">There are two problems that widely exist in current
										end-to-end sign language processing architecture.
										One is the CTC spike phenomenon which weakens the visual representational ability in Continuous
										Sign Language Recognition (CSLR). The other one is the exposure bias problem which leads to
										the accumulation of translation errors during inference in Sign Language Translation (SLT). In this paper, we tackle these issues by introducing contrast
										learning, aiming to enhance both visual-level feature representation and semantic-level error tolerance.
										Specifically, to alleviate CTC spike phenomenon and enhance visual-level representation,
										we design a visual contrastive loss by minimizing visual feature distance between different augmented
										samples of frames in one sign video, so that the model can further explore features by utilizing
										numerous unlabeled frames in an unsupervised way. To alleviate exposure bias problem and
										improve semantic-level error tolerance, we design a semantic contrastive loss by re-inputting the predicted
										sentence into semantic module and comparing features of ground-truth sequence and predicted
										sequence, for exposing model to its own mistakes. Besides, we propose two new metrics, i.e., Blank
										Rate and Consecutive WrongWord Rate to directly reflect our improvement on the two problems. Extensive experimental results on current sign language datasets demonstrate the effectiveness of our approach, which achieves state-of-the-art performance.
									</p>
								</div>
							</div>
						</div>
					</div>

					<div class="row" style="display: flex;align-items: center;">
						<div class="col-md-4 col-sm-6">
							<div class="blog-entry">
								<a href="https://dl.acm.org/doi/10.1145/3581783.3611820" target="_blank" class="blog-img"><img src="images/pgmjoin.png" class="img-responsive" alt="HTML5 Bootstrap Template by colorlib.com"></a>
							</div>
						</div>
						<div class="col-md-8 col-sm-6">
							<div class="blog-entry">
								
								<div class="desc">
									<h3><a href="https://dl.acm.org/doi/10.1145/3581783.3611820" target="_blank">To provide instant communication for hearing-impaired people,
										it is essential to achieve real-time sign language processing anytime
										anywhere. Therefore, in this paper, we propose a Region-aware Temporal Graph based neural Network (RTG-Net), aiming
										to achieve real-time Sign Language Recognition (SLR) and Translation (SLT) on edge devices. To reduce the computation overhead,
										we first construct a shallow graph convolution network to reduce model size by decreasing model depth. Besides, we apply structural
										re-parameterization to fuse the convolutional layer, batch normalization layer and all branches to simplify model complexity
										by reducing model width. To achieve the high performance in sign language processing as well, we extract key regions based
										on keypoints in skeleton from each frame, and design a region-aware temporal graph to combine key regions and full frame for feature representation. In RTG-Net, we design a multi-stage training
										strategy to optimize keypoint selection, SLR and SLT step by step. Experimental results demonstrate that RTG-Net achieves comparable
										performance with existing methods in SLR or SLT, while greatly reducing the computation overhead and achieving real-time
										sign language processing on edge devices. Our code is available at https://github.com/SignLanguageCode/realtimeSLRT.
									</p>
								</div>
							</div>
						</div>
					</div>

					<div class="row" style="display: flex;align-items: center;">
						<div class="col-md-4 col-sm-6">
							<div class="blog-entry">
								<a href="https://dl.acm.org/doi/10.1145/3474085.3475577" target="_blank" class="blog-img"><img src="images/dbestplus.png" class="img-responsive" alt="HTML5 Bootstrap Template by colorlib.com"></a>
							</div>
						</div>
						<div class="col-md-8 col-sm-6">
							<div class="blog-entry">
								
								<div class="desc">
									<h3><a href="https://dl.acm.org/doi/10.1145/3474085.3475577" target="_blank">As an essential communication way for deaf-mutes, sign languages
										are expressed by human actions. To distinguish human actions for sign language understanding, the skeleton which contains position
										information of human pose can provide an important cue, since different actions usually correspond to different poses/skeletons.
										However, skeleton has not been fully studied for Sign Language Translation (SLT), especially for end-to-end SLT. Therefore, in this
										paper, we propose a novel end-to-end Skeleton-Aware neural Network (SANet) for video-based SLT. Specifically, to achieve end-toend SLT, we design a self-contained branch for skeleton extraction.
										To efficiently guide the feature extraction from video with skeletons, we concatenate the skeleton channel and RGB channels of
										each frame for feature extraction. To distinguish the importance of clips, we construct a skeleton-based Graph Convolutional Network
										(GCN) for feature scaling, i.e., giving importance weight for each clip. The scaled features of each clip are then sent to a decoder
										module to generate spoken language. In our SANet, a joint training strategy is designed to optimize skeleton extraction and sign language translation jointly. Experimental results on two large scale
										SLT datasets demonstrate the effectiveness of our approach, which outperforms the state-of-the-art methods. Our code is available at
										https://github.com/SignLanguageCode/SANet.
									</p>
								</div>
							</div>
						</div>
					</div>
 

				</div>
			</section>
			

			<section class="colorlib-education" data-section="education">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3">
							<h2 class="colorlib-heading animate-box">Education</h2>
						</div>
					</div>
					<div class="row">
						<div class="col-md-12">
							<div class="fancy-collapse-panel">
								<div class="panel-group" id="accordion" role="tablist" aria-multiselectable="true">
									<div class="panel panel-default">
									   
										<div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
									        <div class="panel-body">
									            <div class="row">
													<div class="col-md-4">
														<p>Uni: Nanjing University, China </p>
													</div>
													<div class="col-md-8">
														<p>Thesis: Sign Language Recognition and Translation</p>
													</div>
												</div>
									        </div>
									    </div>
									</div> 
									    <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
									        <div class="panel-body">
									            <div class="row">
													<div class="col-md-4">
														<p>Uni: Nanjing University, China </p>
													</div>
													<div class="col-md-8">
														<p>Thesis: Sign Language Recognition and Translation</p>
													</div>
												</div>
									        </div>
									    </div>
									</div>
									<div class="panel panel-default">
									    <div class="panel-heading" role="tab" id="headingThree">
									        <h4 class="panel-title">
									            <a class="collapsed" data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="false" aria-controls="collapseTwo">
													BSc. in Computer Science
									            </a>
									        </h4>
									    </div>
									    <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
									        <div class="panel-body">
									            <div class="row">
													<div class="col-md-4">
														<p>Uni: Hunan University, China </p>
													</div>
													<div class="col-md-8">
														<p>Thesis: Image Deraining</p>
													</div>
												</div>
									        </div>
									    </div>
									</div>
								</div>
							</div>
						</div>
					</div>
				</div>
			</section>

			<section class="colorlib-experience" data-section="experience">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3">
							<h2 class="colorlib-heading">Experience</h2>
						</div>
					</div>
					<div class="row">
						<div class="col-md-12">
				         <div class="timeline-centered">
					         <article class="timeline-entry">
					            <div class="timeline-entry-inner">

					               <div class="timeline-icon color-1">
					                  <i class="icon-pen2"></i>
					               </div>

					               <div class="timeline-label">
					                  <h2>Teaching Assistant</h2>
									  <h3>University of Warwick <span>2022-2023</span></h3>
					                  <p>Teaching Assistant for Image , Relational Databases, and Advanced Databases courses.
										Proficient in guiding students through complex AI and data science concepts while providing collaborative seminar and lab environments. 
										Facilitated discussions and problem-solving sessions, fostering practical application of AI/ML techniques.</p>
					               </div>
					            </div>
					         </article>


					         <article class="timeline-entry">
					            <div class="timeline-entry-inner">
					               <div class="timeline-icon color-2">
					                  <i class="icon-pen2"></i>
					               </div>
					               <div class="timeline-label">
					               	<h2>Big Data Engineer</h2>
									<h3>at ICT Research Institute<span>2019-2020</span></h3>
					                  <p>Collaborated within a team to orchestrate end-to-end data workflows, transforming diverse data sources into Hadoop clusters. 
										Evaluated migration strategies and designed Unix/Linux ETL pipelines for processing Big Data. Enhanced data warehousing and business intelligence solutions.

									  </p>
					               </div>
					            </div>
					         </article>

					         <article class="timeline-entry">
					            <div class="timeline-entry-inner">
					               <div class="timeline-icon color-3">
					                  <i class="icon-pen2"></i>
					               </div>
					               <div class="timeline-label">
					               	<h2>Data and Machine Learning Engineer</h2>
									<h3>Refah Retail Chain Stores Co.<span>2017-2019</span></h3>
					                  <p>Pioneered the creation of an AI-powered retail suite at Refah, implementing cutting-edge techniques such as customer counting, crowded zone detection, and customer
										behavior analysis. Leveraged machine learning to categorize products and classify
										social media comments, providing comprehensive insights for store operations and
										customer interactions
									  </p>
					               </div>
					            </div>
					         </article>

					         <article class="timeline-entry">
					            <div class="timeline-entry-inner">
					               <div class="timeline-icon color-4">
					                  <i class="icon-pen2"></i>
					               </div>
					               <div class="timeline-label">
					               	<h2>Deep Learning Engineer</h2>
									<h3>Sensifai <span>2017-2018</span></h3>
					                  <p>Contributed to the development of a sophisticated audio analysis system, utilizing
										transfer learning for acoustic scene detection, CNN-based mood classification, and
										weakly-labeled human speech recognition. Engineered a parallelized YouTube dataset
										crawler and preprocessing pipeline
									  </p>
					               </div>
					            </div>
					         </article>


					         <article class="timeline-entry begin">
					            <div class="timeline-entry-inner">
					               <div class="timeline-icon color-none">
					               </div>
					            </div>
					         </article>
					      </div>
					   </div>
				   </div>
				</div>
			</section>




		</div><!-- end:colorlib-main -->
	</div><!-- end:container-wrap -->
	</div><!-- end:colorlib-page -->

	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Flexslider -->
	<script src="js/jquery.flexslider-min.js"></script>
	<!-- Owl carousel -->
	<script src="js/owl.carousel.min.js"></script>
	<!-- Counters -->
	<script src="js/jquery.countTo.js"></script>
	
	
	<!-- MAIN JS -->
	<script src="js/main.js"></script>

	</body>
</html>

