<!DOCTYPE html>
<html lang="en">
	<head>
	<meta name="google-site-verification" content="KJ26zwZ4FjKawfaoNocz0bP16LOuH4hazpK3DEeg04Y" />

	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Shiwei Gan</title>
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="" />
	<meta name="keywords" content="" />
	<meta name="author" content="" />

  <!-- Facebook and Twitter integration -->
	<meta property="og:title" content=""/>
	<meta property="og:image" content=""/>
	<meta property="og:url" content=""/>
	<meta property="og:site_name" content=""/>
	<meta property="og:description" content=""/>
	<meta name="twitter:title" content="" />
	<meta name="twitter:image" content="" />
	<meta name="twitter:url" content="" />
	<meta name="twitter:card" content="" />

	<!-- Place favicon.ico and apple-touch-icon.png in the root directory -->
	<link rel="shortcut icon" href="favicon.ico">

	<link href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700" rel="stylesheet">
	
	
	<!-- Animate.css -->
	<link rel="stylesheet" href="css/animate.css">
	<!-- Icomoon Icon Fonts-->
	<link rel="stylesheet" href="css/icomoon.css">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<link rel="stylesheet" href="https://i.icomoon.io/public/temp/c3b0292278/UntitledProject/style.css">

	<!-- Bootstrap  -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<!-- Flexslider  -->
	<link rel="stylesheet" href="css/flexslider.css">
	<!-- Flaticons  -->
	<link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
	<!-- Owl Carousel -->
	<link rel="stylesheet" href="css/owl.carousel.min.css">
	<link rel="stylesheet" href="css/owl.theme.default.min.css">
	<!-- Theme style  -->
	<link rel="stylesheet" href="css/style.css">

	<!-- Modernizr JS -->
	<script src="js/modernizr-2.6.2.min.js"></script>
	<!-- FOR IE9 below -->
	<!--[if lt IE 9]>
	<script src="js/respond.min.js"></script>
	<![endif]-->

	</head>
	<body>
	<div id="colorlib-page">
		<div class="container-wrap">
		<a href="#" class="js-colorlib-nav-toggle colorlib-nav-toggle" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><i></i></a>
		<aside id="colorlib-aside" role="complementary" class="border js-fullheight">
			<div class="text-center">
				<!-- <div class="author-img" style="background-image: url(images/profile_formal.png);"></div>-->
				<a href="images/profile_formal.png" target="_blank">
			      	  <div class="author-img" style="background-image: url(images/profile_formal.png);"></div>
			    	</a>
				<h1 id="colorlib-logo"><a href="index.html">Shiwei Gan</a></h1>
				<span class="position"><a href="#">Machine Learning & Computer Vision</a></span>
			</div>
			<nav id="colorlib-main-menu" role="navigation" class="navbar">
				<div id="navbar" class="collapse">
					<ul>
						<!--<li class="active"><a href="#" data-nav-section="home">Home</a></li>-->
						<li><a href="#" data-nav-section="about">About</a></li>
						<li><a href="#" data-nav-section="blog">Research</a></li>
						<li><a href="#" data-nav-section="education">Education</a></li>
						<li><a href="#" data-nav-section="experience">Experience</a></li>
						
					</ul>
				</div>
			</nav>
			<div class="text-center social-media">
				<ul>
					<li><a href="mailto:sw@smail.nju.edu.cn?cc=gswycf@qq.com"><i class="icon-mail5"></i></a></li>
					<li><a href="static/shiweiCV.pdf" target="_blank"><i class="ai ai-cv"></i></a></li>
					<li><a href="https://scholar.google.com/citations?user=qXmzwiQAAAAJ&hl=zh-CN" target="_blank"><i class="ai ai-google-scholar-square"></i></a></li>
					<li><a href="https://www.linkedin.com/in/shiwei-gan-34491a18b/" target="_blank"><i class="icon-linkedin2"></i></a></li>
					<li><a href="https://github.com/gswycf" target="_blank"><i class="icon-github"></i></a></li>
					<li><a href="https://twitter.com/gswycf" target="_blank"><i class="icon-twitter2"></i></a></li>
				</ul>
			</div>


			<div class="colorlib-footer" style="position: absolute; bottom: 0;">
				<p><small>&copy; <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
Copyright <script>document.write(new Date().getFullYear());</script> All rights reserved. Made by Shiwei Gan
<!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. --> </span> <span>Distributed by Shiwei Gan</span>
				<!--<ul>
					<li><a href="#"><i class="icon-facebook2"></i></a></li>
					<li><a href="#"><i class="icon-twitter2"></i></a></li>
					<li><a href="#"><i class="icon-instagram"></i></a></li>
					<li><a href="#"><i class="icon-linkedin2"></i></a></li>
				</ul>-->
			</div>

		</aside>

		<div id="colorlib-main">

			<section class="colorlib-about" data-section="about">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-12">
							<div class="row row-bottom-padded-sm">
								<div class="col-md-12">
									<div class="about-desc">
										<!--<span class="heading-meta">About Me</span>-->
										<h2 class="colorlib-heading">About Me</h2>
										<!--<p style="font-size: 18px;"><strong>Hi! I'm Shiwei Gan</strong> -->
										<p style="font-size: 16px; text-align: justify;">I am a PhD candidate at the Nanjing University, specializing in Deep Learning and Computer Vision. 
											I work under the supervision of Prof.Sanglu LU and  assistant professor Yafeng Yin.
											My PhD research centers on Sign Language Recognition and Sign Language Translation, encompassing two primary domains: Computer Vision and Multimodal Learning. 
											Broadly speaking, my investigations aim to convert video sequences into text, and to learn human acition representations from videos with CV model and NLP model.
											I am also interest in LVMs.
									</div>
								</div>
							</div>

						</div>
					</div>
				<div class="row row-bottom-padded-sm">
					<div class="news">
						<div class="col-md-12">
							<h2 class="colorlib-heading">News</h2>
							<ul>
								<li><b>May 2024:</b> One paper is accepted at CVPR '24 <a href="https://openreview.net/group?id=thecvf.com/CVPR/2024/Conference#tab-recent-activity" target="_blank">pdf</a></li> </li>
								<li><b>January 2024:</b> One paper is accepted at ACM IMWUT/Ubicomp'24 <a href="https://dl.acm.org/doi/10.1145/3631445" target="_blank">pdf</a></li>
								<li><b>August 2023:</b> One paper is accepted at IJCAI '23 <a href="https://www.ijcai.org/proceedings/2023/0085.pdf" target="_blank">pdf</a></li>
								<li><b>September 2023:</b> One paper is accepted at ACM MM '23 <a href="https://dl.acm.org/doi/10.1145/3581783.3611820" target="_blank">pdf</a></li>
								<li><b>October 2021:</b> One paper is accepted at ACM MM '21 <a href="https://dl.acm.org/doi/10.1145/3474085.3475577" target="_blank">pdf</a></li>
							</ul>
						</div>
					</div>
				</div>

				</div>
			</section>


<section class="colorlib-blog" data-section="blog">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3">
							<h2 class="colorlib-heading">Research</h2>
						</div>
					</div>
					<div class="row" style="display: flex;align-items: center;">
						<div class="col-md-4 col-sm-6">
							<div class="blog-entry">
								<a href="https://openreview.net/group?id=thecvf.com/CVPR/2024/Conference#tab-recent-activity" target="_blank" class="blog-img"><img src="images/cvprintro.png" class="img-responsive" alt="HTML5 Bootstrap Template by colorlib.com"></a>
								<a href="https://openreview.net/group?id=thecvf.com/CVPR/2024/Conference#tab-recent-activity" target="_blank" class="blog-img"><img src="images/cvprmodel.png" class="img-responsive" alt="HTML5 Bootstrap Template by colorlib.com"></a>
							
							</div>
						</div>
						<div class="col-md-8 col-sm-6">
							<div class="blog-entry">
								
								<div class="desc">
									<h3>We propose to represent a sign sequence as graphs and introduce a simple yet 
										effective graph-based sign language processing architecture named SignGraph, 
										to extract cross-region features at the graph level. SignGraph consists of two basic modules: 
										Local Sign Graph (LSG) module for learning the correlation of <b>intra-frame cross-region</b>
										features in one frame and Temporal Sign Graph (TSG) module for tracking the interaction of 
										<b>inter-frame cross-region</b> features among adjacent frames. With LSG and TSG, 
										we build our model in a multiscale manner to ensure that the representation of nodes 
										can capture cross-region features at different granularities. Extensive experiments on current 
										public sign language datasets demonstrate the superiority of our SignGraph model. Our model achieves very
										competitive performances with the SOTA model, while not using any extra cues. Code and models are available at: <a href="https://github.com/gswycf/SignGraph" target="_blank"><i class="icon-github"></i></a>
									</p>
								</div>
							</div>
						</div>
					</div>

					<div class="row" style="display: flex;align-items: center;">
						<div class="col-md-4 col-sm-6">
							<div class="blog-entry">
								<a href="https://www.ijcai.org/proceedings/2023/0085.pdf" target="_blank" class="blog-img"><img src="images/ijcai1.png" class="img-responsive" alt="HTML5 Bootstrap Template by colorlib.com"></a>
								<a href="https://www.ijcai.org/proceedings/2023/0085.pdf" target="_blank" class="blog-img"><img src="images/ijcai2.png" class="img-responsive" alt="HTML5 Bootstrap Template by colorlib.com"></a>
							
							</div>
						</div>
						<div class="col-md-8 col-sm-6">
							<div class="blog-entry">
								
								<div class="desc">
									<h3>In this paper, we tackle  CTC spike phenomenon and  exposure bias by introducing contrast
										learning for CSLR and SLT, aiming to enhance both visual-level feature representation and semantic-level error tolerance.
										Specifically, to alleviate CTC spike phenomenon and enhance visual-level representation,
										we design a visual contrastive loss by minimizing visual feature distance between different augmented
										samples of frames in one sign video, so that the model can further explore features by utilizing
										numerous unlabeled frames in an unsupervised way. To alleviate exposure bias problem and
										improve semantic-level error tolerance, we design a semantic contrastive loss by re-inputting the predicted
										sentence into semantic module and comparing features of ground-truth sequence and predicted
										sequence, for exposing model to its own mistakes. Besides, we propose two new metrics, i.e., Blank
										Rate and Consecutive WrongWord Rate to directly reflect our improvement on the two problems. 
									</p>
								</div>
							</div>
						</div>
					</div>

					<div class="row" style="display: flex;align-items: center;">
						<div class="col-md-4 col-sm-6">
							<div class="blog-entry">
								<a href="https://dl.acm.org/doi/10.1145/3581783.3611820" target="_blank" class="blog-img"><img src="images/mm23-1.png" class="img-responsive" alt="HTML5 Bootstrap Template by colorlib.com"></a>
							</div>
						</div>
						<div class="col-md-8 col-sm-6">
							<div class="blog-entry">
								
								<div class="desc">
									<h3><a href="https://dl.acm.org/doi/10.1145/3581783.3611820" target="_blank">To achieve real-time sign language processing anytime
										  in this paper, we propose a Region-aware Temporal Graph based neural Network (RTG-Net), aiming
										to achieve real-time CSLR SLT on edge devices. To reduce the computation overhead,
										we first construct a shallow graph convolution network to reduce model size by decreasing model depth. Besides, we apply structural
										re-parameterization to fuse the convolutional layer, batch normalization layer and all branches to simplify model complexity
										by reducing model width. To achieve the high performance in sign language processing as well, we extract key regions based
										on keypoints in skeleton from each frame, and design a region-aware temporal graph to combine key regions and full frame for feature representation.  Experimental results demonstrate that RTG-Net achieves comparable
										performance with existing methods in SLR or SLT, while greatly reducing the computation overhead and achieving real-time
										sign language processing on edge devices. 
									</p>
								</div>
							</div>
						</div>
					</div>

					<div class="row" style="display: flex;align-items: center;">
						<div class="col-md-4 col-sm-6">
							<div class="blog-entry">
								<a href="https://dl.acm.org/doi/10.1145/3474085.3475577" target="_blank" class="blog-img"><img src="images/mm21-1.png" class="img-responsive" alt="HTML5 Bootstrap Template by colorlib.com"></a>
							</div>
						</div>
						<div class="col-md-8 col-sm-6">
							<div class="blog-entry">
								
								<div class="desc">
									<h3>As an essential communication way for deaf-mutes, sign languages
										are expressed by human actions. To distinguish human actions for sign language understanding, the skeleton which contains position
										information of human pose can provide an important cue, since different actions usually correspond to different poses/skeletons.
										However, skeleton has not been fully studied for Sign Language Translation (SLT), especially for end-to-end SLT. Therefore, in this
										paper, we propose a novel end-to-end Skeleton-Aware neural Network (SANet) for video-based SLT. Specifically, to achieve end-toend SLT, we design a self-contained branch for skeleton extraction.
										To efficiently guide the feature extraction from video with skeletons, we concatenate the skeleton channel and RGB channels of
										each frame for feature extraction. To distinguish the importance of clips, we construct a skeleton-based Graph Convolutional Network
										(GCN) for feature scaling, i.e., giving importance weight for each clip. The scaled features of each clip are then sent to a decoder
										module to generate spoken language. In our SANet, a joint training strategy is designed to optimize skeleton extraction and sign language translation jointly.  
									</p>
								</div>
							</div>
						</div>
					</div>
				</div>
</section>
			

			<section class="colorlib-education" data-section="education">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3">
							<h2 class="colorlib-heading animate-box">Education</h2>
						</div>
					</div>
					<div class="row">
						<div class="col-md-12">
							<div class="fancy-collapse-panel">
								<div class="panel-group" id="accordion" role="tablist" aria-multiselectable="true">
									<div class="panel panel-default">
									    <div class="panel-heading" role="tab" id="headingOne">
									        <h4 class="panel-title">
									            <a data-toggle="collapse" data-parent="#accordion" href="#collapseOne" aria-expanded="true" aria-controls="collapseOne">
													PhD in Computer Science, The University of Warwick, Coventry, UK
									            </a>
									        </h4>
									    </div>
									    <div id="collapseOne" class="panel-collapse collapse in" role="tabpanel" aria-labelledby="headingOne">
									         <div class="panel-body">
									            <div class="row">
										      		<div class="col-md-4">
										      			<p> Visiting student sponsored by CSC Scholarship </p>
										      		</div>
										      		<div class="col-md-8">
										      			<p>Advised by Professor <a href="https://hongkaiw.github.io/" target="_blank">Hongkai Wen</a></p>
										      		</div>
										      	</div>
									         </div>
									    </div>
									</div>
									<div class="panel panel-default">
									    <div class="panel-heading" role="tab" id="headingTwo">
									        <h4 class="panel-title">
									            <a class="collapsed" data-toggle="collapse" data-parent="#accordion" href="#collapseTwo" aria-expanded="false" aria-controls="collapseTwo">
													PhD in Computer Science, Nanjing University, Nanjing, China
									            </a>
									        </h4>
									    </div>
									    <div id="collapseTwo" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingTwo">
									        <div class="panel-body">
									            <div class="row">
													<div class="col-md-8">
														<p> Advised by Professor <a href="https://cs.nju.edu.cn/58/1e/c2639a153630/pagem.htm" target="_blank">Sanglu Lu</a> and Associate Researcher <a href="https://yafengnju.github.io/" target="_blank">Yafeng Yin</a>
										      		</div>
													<div class="col-md-8">
														<p>Thesis: Sign Language Recognition and Translation</p>
													</div>
												</div>
									        </div>
									    </div>
									</div>
									<div class="panel panel-default">
									    <div class="panel-heading" role="tab" id="headingThree">
									        <h4 class="panel-title">
									            <a class="collapsed" data-toggle="collapse" data-parent="#accordion" href="#collapseThree" aria-expanded="false" aria-controls="collapseTwo">
													BSc. in Computer Science, Hunan University, Changsha, China
									            </a>
									        </h4>
									    </div>
									    <div id="collapseThree" class="panel-collapse collapse" role="tabpanel" aria-labelledby="headingThree">
									        <div class="panel-body">
									            <div class="row"> 
													<div class="col-md-4">
														<p>Average score: 87.26/100. </p>
													</div>
													<div class="col-md-4">
														<p>  Rank: 4/184 (Top 3.26%)</p>
													</div> 
												</div>
									        </div>
									    </div>
									</div>
								</div>
							</div>
						</div>
					</div>
				</div>
			</section>

			<section class="colorlib-experience" data-section="experience">
				<div class="colorlib-narrow-content">
					<div class="row">
						<div class="col-md-6 col-md-offset-3 col-md-pull-3">
							<h2 class="colorlib-heading">Experience</h2>
						</div>
					</div>
					<div class="row">
						<div class="col-md-12">
				         <div class="timeline-centered">
							<article class="timeline-entry">
					            <div class="timeline-entry-inner">

					               <div class="timeline-icon color-1">
					                  <i class="icon-pen2"></i>
					               </div>

					               <div class="timeline-label">
					                  <h2>Reviewer</h2>
									  <h3> Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies(IMWUT)<span>2024</span></h3>
					                  <h3>ACM Multimedia<span>2021, 2023, 2024</span></h3>
									  <h3>AAAI Conference on Artificial Intelligence<span>2021</span></h3>
					               </div>
					            </div>
					         </article>

					         <article class="timeline-entry">
					            <div class="timeline-entry-inner">

					               <div class="timeline-icon color-1">
					                  <i class="icon-pen2"></i>
					               </div>

					               <div class="timeline-label">
					                  <h2>Invigilator</h2>
									  <h3>The University of Warwick <span>2024</span></h3>
					                  
					               </div>
					            </div>
					         </article>
					         <article class="timeline-entry">
					            <div class="timeline-entry-inner">
					               <div class="timeline-icon color-4">
					                  <i class="icon-pen2"></i>
					               </div>
					               <div class="timeline-label">
					               	<h2>Teaching Assistant</h2>
									<h3>Course: CS933-15 Image and Video Analysis,  The University of Warwick<span>Fall 2023</span></h3>
									<h3>Course: Computer Network,  Nanjing University<span>Spring 2024</span></h3>
									
									<h3>Course: Data Structure,  Nanjing University <span>2019-2022</span></h3>
					    
					               </div>
					            </div>
					         </article>


					         <article class="timeline-entry begin">
					            <div class="timeline-entry-inner">
					               <div class="timeline-icon color-none">
					               </div>
					            </div>
					         </article>
					      </div>
					   </div>
				   </div>
				</div>
			</section>

			<a href="https://info.flagcounter.com/prqj"><img src="https://s11.flagcounter.com/count2/prqj/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>
			<a href="https://clustrmaps.com/site/1c023"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=AFzVfj9ceGZAz2fUKU6I9AiCMZO7ehwHM5N2MEmuAQ0&cl=ffffff" /></a>
		</div><!-- end:colorlib-main -->
	</div><!-- end:container-wrap -->
	</div><!-- end:colorlib-page -->

	<!-- jQuery -->
	<script src="js/jquery.min.js"></script>
	<!-- jQuery Easing -->
	<script src="js/jquery.easing.1.3.js"></script>
	<!-- Bootstrap -->
	<script src="js/bootstrap.min.js"></script>
	<!-- Waypoints -->
	<script src="js/jquery.waypoints.min.js"></script>
	<!-- Flexslider -->
	<script src="js/jquery.flexslider-min.js"></script>
	<!-- Owl carousel -->
	<script src="js/owl.carousel.min.js"></script>
	<!-- Counters -->
	<script src="js/jquery.countTo.js"></script>
	
	
	<!-- MAIN JS -->
	<script src="js/main.js"></script>

	</body>
</html>

